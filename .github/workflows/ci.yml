name: CI

on:
  push:
    branches-ignore:
      - 'compare-baseline-*'
  pull_request:

permissions:
  actions: write

env:
  PYTHONUNBUFFERED: '1'
  CCACHE_BASEDIR: ${{ github.workspace }}
  CCACHE_DIR: ${{ github.workspace }}/.ccache
  CCACHE_MAXSIZE: 400M
  CCACHE_SLOPPINESS: time_macros

jobs:
  # Get the LFS cache ready. This avoids every shard of each environment downloading the LFS objects
  # at the same time on any change, which is bad for quota.
  warm-lfs-cache:
    runs-on: ubuntu-latest
    steps:
      - name: git clone
        uses: connorjclark/action-cached-lfs-checkout@e9e548e0068e62830582e21febff8da4bd4e8ca5

  test:
    needs: warm-lfs-cache
    strategy:
      matrix:
        include:
          # - runs-on: windows-2022
          #   arch: x64
          #   compiler: msvc
          #   # RelWithDebInfo invalidates ccache, and we currently can't show backtraces on crash for windows in CI
          #   # so no need for it yet.
          #   config: Release
          # - runs-on: windows-2022
          #   arch: win32
          #   compiler: msvc
          #   config: RelWithDebInfo
          #   extra-args: --max_duration 60
          # - runs-on: macos-13
          #   arch: intel
          #   compiler: clang
          #   config: RelWithDebInfo
          - runs-on: ubuntu-22.04
            arch: x64
            compiler: clang
            config: RelWithDebInfo
      fail-fast: false
    # See https://github.com/community/community/discussions/40777
    uses: ./.github/workflows/test.yml
    # uses: ZQuestClassic/ZQuestClassic/.github/workflows/test.yml@main
    with:
      runs-on: ${{ matrix.runs-on }}
      arch: ${{ matrix.arch }}
      compiler: ${{ matrix.compiler }}
      config: ${{ matrix.config }}
      extra-args: ${{ matrix.extra-args }} --prune_test_results
      from-ci-workflow: true

  compare:
    needs: ["test"]
    runs-on: ubuntu-latest
    if: failure()
    env:
      REPLAY_FAILURE_DISCORD_WEBHOOK: ${{ secrets.REPLAY_FAILURE_DISCORD_WEBHOOK }}

    steps:
      - name: Install Node.js 18.x
        uses: actions/setup-node@v3
        with:
          node-version: 18.x
      - name: git clone
        uses: actions/checkout@v4
        with:
          # Need history to find baseline commit.
          fetch-depth: 0
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11.4'
          cache: pip
      - run: pip install -r requirements.txt

      - uses: actions/download-artifact@v4
        with:
          path: test-results
          pattern: replays-*

      # TODO: instead of starting baseline workflow runs when all platforms finish,
      # kick each off as necessary in `test.yml` and just poll for their completion here.
      - name: Collect baseline
        run: python3 tests/run_test_workflow.py --test_results test-results --repo ${{ github.repository }} --token ${{ secrets.GITHUB_TOKEN }}
        id: collect-baseline
      - name: Generate report
        run: python3 tests/compare_replays.py --workflow_run ${{ steps.collect-baseline.outputs.baseline_run_id }} --local test-results --repo ${{ github.repository }} --token ${{ secrets.GITHUB_TOKEN }}

      - run: npx surge ${{ github.workspace }}/tests/compare-report zc-replay-compare-${{ github.run_id }}.surge.sh --token ${{ secrets.SURGE_TOKEN }}
        name: Upload to https://zc-replay-compare-${{ github.run_id }}.surge.sh

      - name: Discord notification
        shell: python
        run: |
          import inspect
          import discord

          url = '${{ secrets.REPLAY_FAILURE_DISCORD_WEBHOOK }}'
          ref_name = '${{ github.ref_name }}'
          run_id = '${{ github.run_id }}'
          repository = '${{ github.repository }}'
          actor = '${{ github.actor }}'

          content = inspect.cleandoc(f'''
              Report: https://zc-replay-compare-{run_id}.surge.sh
              CI: https://github.com/{repository}/actions/runs/{run_id}
              Actor: {actor}
          ''')

          webhook = discord.SyncWebhook.from_url(url)
          webhook.send(
              content,
              thread_name=f'Replay tests failed for {ref_name}',
              wait=True,
          )

      # Upload as artifact, so that if publishing to surge fails the report is still accessible.
      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: report
          path: ${{ github.workspace }}/tests/compare-report

  web:
    needs: warm-lfs-cache
    runs-on: ubuntu-22.04

    steps:
      - name: git clone
        uses: connorjclark/action-cached-lfs-checkout@e9e548e0068e62830582e21febff8da4bd4e8ca5
        with:
          fetch-depth: 0

      - uses: mymindstorm/setup-emsdk@v12
        with:
          version: "3.1.64"
          actions-cache-folder: "emsdk-cache-${{ runner.os }}-${{ runner.arch }}"

      - name: Install Node.js 18.x
        uses: actions/setup-node@v3
        with:
          node-version: 18.x
      # puppeteer caches Chrome in ~/.ccache/puppeteer, but only on npm install. Which means
      # restored caches from actions/setup-node will never install Chrome.
      - run: npm rm puppeteer && npm add puppeteer@21.1.1

      - run: sudo apt-get update && sudo apt-get install ccache ninja-build flex bison

      # Setup build cache via ccache.
      - name: ccache cache files
        uses: actions/cache@v4
        with:
          path: .ccache
          key: web-ccache-v2-${{ github.run_id }}
          restore-keys: web-ccache-v2-
      - run: ccache -z

      - run: |
          git clone https://github.com/psi29a/unsf.git
          cd unsf
          cmake -S . -B build .
          cmake --build build
          echo "UNSF=$PWD/build/unsf-static" >> $GITHUB_ENV

      - run: |
          git config --global user.email "you@example.com"
          git config --global user.name "Your Name"
      - run: bash scripts/configure_emscripten.sh
        env:
          ZC_EMCC_CMAKE_EXTRA_FLAGS: -DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache
      - run: cmake --build build_emscripten --config RelWithDebInfo -t web
        env:
          ZC_PACKAGE_REPLAYS: 1
      - run: ccache -s

      - run: sudo apt-get update && sudo apt-get install libopengl0 libglu1
      - run: sudo apt-get install xvfb
      - run: npm install
        working-directory: web
      - run: xvfb-run --auto-servernum npm run test -- --timeout 100000 || xvfb-run --auto-servernum npm run test -- --timeout 100000 || xvfb-run --auto-servernum npm run test -- --timeout 100000
        working-directory: web
        env:
          BUILD_FOLDER: ../build_emscripten/RelWithDebInfo
      - run: pip install -r requirements.txt
      # Run each replay for just ~15 seconds, but run all of classic_1st_lvl1.zplay
      # - run: xvfb-run --auto-servernum python -Xutf8 tests/run_replay_tests.py --build_folder build_emscripten/RelWithDebInfo --test_results_folder .tmp/test_results --max_duration 15 --max_duration classic_1st_lvl1.zplay=0 --ci web --retries 2
      # TODO: restore the above, after web replay tests are less flaky / faster.
      - run: xvfb-run --auto-servernum python -Xutf8 tests/run_replay_tests.py --build_folder build_emscripten/RelWithDebInfo --test_results_folder .tmp/test_results --filter classic_1st_lvl1 --filter playground --ci web --retries 2

      - run: BUILD_FOLDER=build_emscripten/RelWithDebInfo xvfb-run --auto-servernum python -Xutf8 tests/test_zeditor.py
      - run: BUILD_FOLDER=build_emscripten/RelWithDebInfo xvfb-run --auto-servernum python -Xutf8 tests/test_zplayer.py

      - uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: emscripten cmake
          path: |
            ${{ github.workspace }}/build_emscripten/CMakeFiles/CMakeOutput.log
            ${{ github.workspace }}/build_emscripten/CMakeFiles/CMakeError.log
      - uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: web test results
          path: ${{ github.workspace }}/.tmp/test_results
